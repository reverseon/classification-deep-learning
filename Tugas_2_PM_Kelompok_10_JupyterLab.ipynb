{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qYSE89k3uEz"
      },
      "source": [
        "# **Tugas 2 MA4072 - Pembelajaran Mendalam**\n",
        "\n",
        "oleh: \\\\\n",
        "Michella Chandra - 10118011 \\\\\n",
        "Thirafi Najwan Kurniatama - 13520157\n",
        "\n",
        "Akan dibuat 4 buah model Deep Learning dengan berbagai kombinasi fungsi aktivasi sebagai berikut: \n",
        "1.   hidden layers memakai fungsi aktivasi **Sigmoid**, output layer memakai fungsi aktivasi **Softmax** dengan fungsi kerugian (*loss function*) berupa **Cross-Entropy (log-loss)**,\n",
        "2.   hidden layers memakai fungsi aktivasi **Tanh**, output layer memakai fungsi aktivasi** Softmax** dengan fungsi kerugian (*loss function*) berupa** Cross-Entropy (log-loss)**,\n",
        "3.   hidden layers memakai fungsi aktivasi **Sigmoid**, output layer memakai fungsi aktivasi **Sigmoid** dengan fungsi biaya (*cost function*) berupa **Mean-Squared Errors (MSE)**, dan\n",
        "4.   hidden layers memakai fungsi aktivasi **Tanh**, output layer memakai fungsi aktivasi **Sigmoid** dengan fungsi biaya (*cost function*) berupa **Mean-Squared Errors (MSE)**.\n",
        "\n",
        "Keempat model ini akan diuji menggunakan dua data Eksperimen. Eksperimen pertama berupa  masalah klasikasi angka,  dengan  data training sebanyak 1.797 observasi dan 64 features(variabel). Eksperimen kedua yaitu klasifikasi jenis pakaian, dengan data training sebanyak 60.000 observasi dan 784 features(variables)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7G4G0J43KN9"
      },
      "source": [
        "## Import Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N5QfDxonrS1e"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss, mean_squared_error\n",
        "from mnist import MNIST\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBlDyr19zyKP"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_HC8aI1z9Y-"
      },
      "source": [
        "Sebagai catatan khusus, nama 'test-images-idx3-ubyte' diganti menjadi 't10k-images-idx3-ubyte' dan 'test-labels-idx1-ubyte' diganti menjadi 't10k-labels-idx1-ubyte'.\n",
        "\n",
        "Jika ingin load data pakaian, silahkan comment cell ini dan uncomment cell di bawahnya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scDiVJ9PrS1h",
        "outputId": "ee0713d5-8e5d-41c3-9090-7a4a0fa44d73"
      },
      "outputs": [],
      "source": [
        "dat = pickle.load(open('digit/digits.dat', \"rb\"))\n",
        "images_tr = dat.data\n",
        "labels_tr_raw = dat.target\n",
        "images_tr = np.array(images_tr)\n",
        "labels_tr_raw = np.array(labels_tr_raw) \n",
        "labels_tr = np.zeros((labels_tr_raw.shape[0], np.amax(labels_tr_raw)+1), dtype=int)\n",
        "for i in range(labels_tr_raw.shape[0]):\n",
        "    labels_tr[i, labels_tr_raw[i]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_mFkuy83bv0t"
      },
      "outputs": [],
      "source": [
        "# dat = MNIST('pakaian')\n",
        "# images_tr, labels_tr_raw = dat.load_training()\n",
        "# images_ts, labels_ts_raw = dat.load_testing()\n",
        "# images_tr = np.array(images_tr)\n",
        "# labels_tr_raw = np.array(labels_tr_raw) \n",
        "# images_ts = np.array(images_ts)\n",
        "# labels_ts_raw = np.array(labels_ts_raw)\n",
        "# labels_ts = np.zeros((labels_ts_raw.shape[0], np.amax(labels_ts_raw)+1), dtype=int)\n",
        "# for i in range(labels_ts_raw.shape[0]):\n",
        "#     labels_ts[i, labels_ts_raw[i]] = 1\n",
        "# labels_tr = np.zeros((labels_tr_raw.shape[0], np.amax(labels_tr_raw)+1), dtype=int)\n",
        "# for i in range(labels_tr_raw.shape[0]):\n",
        "#     labels_tr[i, labels_tr_raw[i]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ltD8APkrS1l"
      },
      "source": [
        "## Feature Normalization / Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81lETxLW3oAf"
      },
      "source": [
        "Berikut adalah kode yang perlu dijalankan ketika feature perlu dinormalisasi/distandarisasi. Apabila ingin dijalankan, silahkan uncomment fungsi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k7nCr6yIrS1o"
      },
      "outputs": [],
      "source": [
        "# NORMALIZATION\n",
        "# images_tr = (images_tr - np.mean(images_tr, axis=1, keepdims=True))/np.std(images_tr, axis=1, keepdims=True)\n",
        "\n",
        "# STANDARDIZATION\n",
        "# images_tr = (images_tr - np.amin(images_tr, axis=1, keepdims=True))/(np.amax(images_tr, axis=1, keepdims=True) - np.amin(images_tr, axis=1, keepdims=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jlmalxMrS1q",
        "outputId": "8df16b29-c665-4668-a9e9-dbf955dd3fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1797, 64)\n",
            "(1797, 10)\n"
          ]
        }
      ],
      "source": [
        "print(images_tr.shape)\n",
        "print(labels_tr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2NEpAS6rS1s"
      },
      "source": [
        "## Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnCsw9mq34Nz"
      },
      "source": [
        "### Fungsi-Fungsi Dasar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-ojgQfLCrS1t"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "def derivtanh(x):\n",
        "    return 1.0 - np.tanh(x)**2\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "def derivsoftmax(x):\n",
        "    return softmax(x) * (1.0 - softmax(x))\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "def dsigmoid(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "def interpretonehot(arr):\n",
        "    return np.argmax(arr, axis=1)\n",
        "def err(l1, l2):\n",
        "    errnum = 0\n",
        "    for i in range(len(l1)):\n",
        "        if l1[i] != l2[i]:\n",
        "            errnum += 1\n",
        "    return errnum/len(l1)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AN9Md0o4QNH"
      },
      "source": [
        "### 1. Fungsi Aktivasi Sigmoid, Softmax untuk Layer Output, dan Loss Function Softmax (Log-Loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qfD85qclrS1w"
      },
      "outputs": [],
      "source": [
        "class DigsNeuralNetworkSigmoidSoftmaxLogloss:\n",
        "    def __init__(self, dataset, oheoutput, flayer, slayer, learningrate):\n",
        "        np.random.seed(23638168)\n",
        "        self.X = dataset\n",
        "        self.alpha = learningrate\n",
        "        self.T = dataset.shape[0]\n",
        "        self.K = dataset.shape[1]\n",
        "        self.H1 = flayer\n",
        "        self.H2 = slayer\n",
        "        self.N0 = oheoutput.shape[1]\n",
        "        self.output = oheoutput\n",
        "        # INITIALIZE FIRST PARAM\n",
        "        limw0 = np.sqrt(6)/np.sqrt(self.K+self.H1)\n",
        "        self.W0 = np.random.uniform(-limw0, limw0, (self.K, self.H1))\n",
        "        # INITIALIZE SECOND PARAM\n",
        "        limw1 = np.sqrt(6)/np.sqrt(self.H1+self.H2)\n",
        "        self.W1 = np.random.uniform(-limw1, limw1, (self.H1, self.H2))\n",
        "        # INITIALIZE THIRD PARAM\n",
        "        limw2 = np.sqrt(6)/np.sqrt(self.H2+self.N0)\n",
        "        self.W2 = np.random.uniform(-limw2, limw2, (self.H2, self.N0))\n",
        "        #NADAM PARAM\n",
        "        self.mW0 = np.zeros((self.K, self.H1))\n",
        "        self.mW1 = np.zeros((self.H1, self.H2))\n",
        "        self.mW2 = np.zeros((self.H2, self.N0))\n",
        "        self.vW0 = np.zeros((self.K, self.H1))\n",
        "        self.vW1 = np.zeros((self.H1, self.H2))\n",
        "        self.vW2 = np.zeros((self.H2, self.N0))\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, dat=None):\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.Z1 = dat@self.W0\n",
        "        self.A1 = sigmoid(self.Z1)\n",
        "        self.Z2 = self.A1@self.W1\n",
        "        self.A2 = sigmoid(self.Z2)\n",
        "        self.Z3 = self.A2@self.W2\n",
        "        self.A3 = softmax(self.Z3)\n",
        "        return self.A3\n",
        "        \n",
        "    def backward(self, dat=None, output=None, epoch=0):\n",
        "        t = dat.shape[0]\n",
        "        if output is None:\n",
        "            output = self.output\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.delta3 = -1/t * (output - self.A3)\n",
        "        self.delta2 = self.delta3 @ self.W2.T * dsigmoid(self.Z2)\n",
        "        self.delta1 = self.delta2 @ self.W1.T * dsigmoid(self.Z1)\n",
        "        dcdw2 = self.A2.T @ self.delta3\n",
        "        dcdw1 = self.A1.T @ self.delta2\n",
        "        dcdw0 = dat.T @ self.delta1\n",
        "        self.mW0 = self.beta1 * self.mW0 + (1 - self.beta1) * dcdw0\n",
        "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dcdw1\n",
        "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dcdw2\n",
        "        self.vW0 = self.beta2 * self.vW0 + (1 - self.beta2) * dcdw0**2\n",
        "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * dcdw1**2\n",
        "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * dcdw2**2\n",
        "        self.mW0hat = self.mW0 / (1 - self.beta1**(epoch+1)) \n",
        "        self.mW1hat = self.mW1 / (1 - self.beta1**(epoch+1))\n",
        "        self.mW2hat = self.mW2 / (1 - self.beta1**(epoch+1))\n",
        "        self.vW0hat = self.vW0 / (1 - self.beta2**(epoch+1))\n",
        "        self.vW1hat = self.vW1 / (1 - self.beta2**(epoch+1))\n",
        "        self.vW2hat = self.vW2 / (1 - self.beta2**(epoch+1))\n",
        "        self.W2 = self.W2 - (self.alpha * self.mW2hat / (np.sqrt(self.vW2hat) + self.epsilon))\n",
        "        self.W1 = self.W1 - (self.alpha * self.mW1hat / (np.sqrt(self.vW1hat) + self.epsilon))\n",
        "        self.W0 = self.W0 - (self.alpha * self.mW0hat / (np.sqrt(self.vW0hat) + self.epsilon))\n",
        "    \n",
        "    def train(self, epochs, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.T\n",
        "        else:\n",
        "            tmp = list(zip(self.X, self.output))\n",
        "            np.random.shuffle(tmp)\n",
        "            self.X, self.output = zip(*tmp)\n",
        "            self.X = np.array(self.X)\n",
        "            self.output = np.array(self.output)\n",
        "        print(\"Hidden Layer Sigmoid, Output Softmax, Log-Loss\")\n",
        "        for i in range(epochs):\n",
        "            for j in range(0, self.T, batch_size):\n",
        "                data = self.X[j:min(j+batch_size, self.T)]\n",
        "                output = self.output[j:min(j+batch_size, self.T)]\n",
        "                self.forward(data)\n",
        "                self.backward(data, output, i)\n",
        "            if (i+1)%(max(epochs//50,1)) == 0:\n",
        "                pred = self.forward(self.X)\n",
        "                print(\"Epoch: \", i+1, \"Cost:\", log_loss(self.output, pred), end=\" \")\n",
        "                print(\"Accuracy:\", \"%.2f\" % (100-err(interpretonehot(self.output), interpretonehot(pred))), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm9fOvyrS1z"
      },
      "source": [
        "### 2. Fungsi Aktivasi Tanh, Softmax untuk Layer Output, dan Loss Function Softmax (Log-Loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "und3jPxDrS10"
      },
      "outputs": [],
      "source": [
        "class DigsNeuralNetworkTanhSoftmaxLogloss:\n",
        "    def __init__(self, dataset, oheoutput, flayer, slayer, learningrate):\n",
        "        np.random.seed(23638168)\n",
        "        self.X = dataset\n",
        "        self.alpha = learningrate\n",
        "        self.T = dataset.shape[0]\n",
        "        self.K = dataset.shape[1]\n",
        "        self.H1 = flayer\n",
        "        self.H2 = slayer\n",
        "        self.N0 = oheoutput.shape[1]\n",
        "        self.output = oheoutput\n",
        "        # INITIALIZE FIRST PARAM\n",
        "        limw0 = np.sqrt(6)/np.sqrt(self.K+self.H1)\n",
        "        self.W0 = np.random.uniform(-limw0, limw0, (self.K, self.H1))\n",
        "        self.b0 = np.random.uniform(-limw0, limw0, (1, self.H1))\n",
        "        # INITIALIZE SECOND PARAM\n",
        "        limw1 = np.sqrt(6)/np.sqrt(self.H1+self.H2)\n",
        "        self.W1 = np.random.uniform(-limw1, limw1, (self.H1, self.H2))\n",
        "        self.b1 = np.random.uniform(-limw1, limw1, (1, self.H2))\n",
        "        # INITIALIZE THIRD PARAM\n",
        "        limw2 = np.sqrt(6)/np.sqrt(self.H2+self.N0)\n",
        "        self.W2 = np.random.uniform(-limw2, limw2, (self.H2, self.N0))\n",
        "        self.b2 = np.random.uniform(-limw2, limw2, (1, self.N0))\n",
        "        #NADAM PARAM\n",
        "        self.mW0 = np.zeros((self.K, self.H1))\n",
        "        self.mW1 = np.zeros((self.H1, self.H2))\n",
        "        self.mW2 = np.zeros((self.H2, self.N0))\n",
        "        self.vW0 = np.zeros((self.K, self.H1))\n",
        "        self.vW1 = np.zeros((self.H1, self.H2))\n",
        "        self.vW2 = np.zeros((self.H2, self.N0))\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, dat=None):\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.Z1 = dat@self.W0\n",
        "        self.A1 = tanh(self.Z1)\n",
        "        self.Z2 = self.A1@self.W1\n",
        "        self.A2 = tanh(self.Z2)\n",
        "        self.Z3 = self.A2@self.W2\n",
        "        self.A3 = softmax(self.Z3)\n",
        "        return self.A3\n",
        "    def backward(self, dat=None, output=None, epoch=0):\n",
        "        t = dat.shape[0]\n",
        "        if output is None:\n",
        "            output = self.output\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.delta3 = -1/t * (output - self.A3)\n",
        "        self.delta2 = self.delta3 @ self.W2.T * derivtanh(self.Z2)\n",
        "        self.delta1 = self.delta2 @ self.W1.T * derivtanh(self.Z1)\n",
        "        dcdw2 = self.A2.T @ self.delta3\n",
        "        dcdw1 = self.A1.T @ self.delta2\n",
        "        dcdw0 = dat.T @ self.delta1\n",
        "        self.mW0 = self.beta1 * self.mW0 + (1 - self.beta1) * dcdw0\n",
        "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dcdw1\n",
        "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dcdw2\n",
        "        self.vW0 = self.beta2 * self.vW0 + (1 - self.beta2) * dcdw0**2\n",
        "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * dcdw1**2\n",
        "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * dcdw2**2\n",
        "        self.mW0hat = self.mW0 / (1 - self.beta1**(epoch+1)) \n",
        "        self.mW1hat = self.mW1 / (1 - self.beta1**(epoch+1))\n",
        "        self.mW2hat = self.mW2 / (1 - self.beta1**(epoch+1))\n",
        "        self.vW0hat = self.vW0 / (1 - self.beta2**(epoch+1))\n",
        "        self.vW1hat = self.vW1 / (1 - self.beta2**(epoch+1))\n",
        "        self.vW2hat = self.vW2 / (1 - self.beta2**(epoch+1))\n",
        "        self.W2 = self.W2 - (self.alpha * self.mW2hat / (np.sqrt(self.vW2hat) + self.epsilon))\n",
        "        self.W1 = self.W1 - (self.alpha * self.mW1hat / (np.sqrt(self.vW1hat) + self.epsilon))\n",
        "        self.W0 = self.W0 - (self.alpha * self.mW0hat / (np.sqrt(self.vW0hat) + self.epsilon))\n",
        "    def train(self, epochs, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.T\n",
        "        else:\n",
        "            tmp = list(zip(self.X, self.output))\n",
        "            np.random.shuffle(tmp)\n",
        "            self.X, self.output = zip(*tmp)\n",
        "            self.X = np.array(self.X)\n",
        "            self.output = np.array(self.output)\n",
        "        print(\"Hidden Layer Tanh, Output Softmax, Log-Loss\")\n",
        "        for i in range(epochs):\n",
        "            for j in range(0, self.T, batch_size):\n",
        "                data = self.X[j:min(j+batch_size, self.T)]\n",
        "                output = self.output[j:min(j+batch_size, self.T)]\n",
        "                self.forward(data)\n",
        "                self.backward(data, output, i)\n",
        "            if (i+1)%(max(epochs//50,1)) == 0:\n",
        "                pred = self.forward(self.X)\n",
        "                print(\"Epoch: \", i+1, \"Cost:\", log_loss(self.output, pred), end=\" \")\n",
        "                print(\"Accuracy:\", \"%.2f\" % (100-err(interpretonehot(self.output), interpretonehot(pred))), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry1c84aprS12"
      },
      "source": [
        "### 3. Fungsi Aktivasi Sigmoid, Sigmoid untuk Layer Output, dan Loss Function MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0ppELvYLrS12"
      },
      "outputs": [],
      "source": [
        "class DigsNeuralNetworkSigmoidMSE:\n",
        "    def __init__(self, dataset, oheoutput, flayer, slayer, learningrate):\n",
        "        np.random.seed(23638168)\n",
        "        self.X = dataset\n",
        "        self.alpha = learningrate\n",
        "        self.T = dataset.shape[0]\n",
        "        self.K = dataset.shape[1]\n",
        "        self.H1 = flayer\n",
        "        self.H2 = slayer\n",
        "        self.N0 = oheoutput.shape[1]\n",
        "        self.output = oheoutput\n",
        "        # INITIALIZE FIRST PARAM\n",
        "        limw0 = np.sqrt(6)/np.sqrt(self.K+self.H1)\n",
        "        self.W0 = np.random.uniform(-limw0, limw0, (self.K, self.H1))\n",
        "        self.b0 = np.random.uniform(-limw0, limw0, (1, self.H1))\n",
        "        # INITIALIZE SECOND PARAM\n",
        "        limw1 = np.sqrt(6)/np.sqrt(self.H1+self.H2)\n",
        "        self.W1 = np.random.uniform(-limw1, limw1, (self.H1, self.H2))\n",
        "        self.b1 = np.random.uniform(-limw1, limw1, (1, self.H2))\n",
        "        # INITIALIZE THIRD PARAM\n",
        "        limw2 = np.sqrt(6)/np.sqrt(self.H2+self.N0)\n",
        "        self.W2 = np.random.uniform(-limw2, limw2, (self.H2, self.N0))\n",
        "        self.b2 = np.random.uniform(-limw2, limw2, (1, self.N0))\n",
        "        #NADAM PARAM\n",
        "        self.mW0 = np.zeros((self.K, self.H1))\n",
        "        self.mW1 = np.zeros((self.H1, self.H2))\n",
        "        self.mW2 = np.zeros((self.H2, self.N0))\n",
        "        self.vW0 = np.zeros((self.K, self.H1))\n",
        "        self.vW1 = np.zeros((self.H1, self.H2))\n",
        "        self.vW2 = np.zeros((self.H2, self.N0))\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, dat=None):\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.Z1 = dat@self.W0 + self.b0\n",
        "        self.A1 = sigmoid(self.Z1)\n",
        "        self.Z2 = self.A1@self.W1 + self.b1\n",
        "        self.A2 = sigmoid(self.Z2)\n",
        "        self.Z3 = self.A2@self.W2 + self.b2\n",
        "        self.A3 = sigmoid(self.Z3)\n",
        "        return self.A3\n",
        "    def backward(self, dat=None, output=None, epoch=0):\n",
        "        if output is None:\n",
        "            output = self.output\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.delta3 = -1/self.T * ((output - self.A3) * dsigmoid(self.Z3))\n",
        "        self.delta2 = (self.delta3 @ self.W2.T) * dsigmoid(self.Z2)\n",
        "        self.delta1 = (self.delta2 @ self.W1.T) * dsigmoid(self.Z1)\n",
        "        dcdw0 = dat.T @ self.delta1\n",
        "        dcdw1 = self.A1.T @ self.delta2\n",
        "        dcdw2 = self.A2.T @ self.delta3\n",
        "        self.mW0 = self.beta1 * self.mW0 + (1 - self.beta1) * dcdw0\n",
        "        self.vW0 = self.beta2 * self.vW0 + (1 - self.beta2) * dcdw0**2\n",
        "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dcdw1\n",
        "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * dcdw1**2\n",
        "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dcdw2\n",
        "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * dcdw2**2\n",
        "        self.mW0hat = self.mW0  / (1 - self.beta1**(epoch+1))\n",
        "        self.mW1hat = self.mW1  / (1 - self.beta1**(epoch+1))\n",
        "        self.mW2hat = self.mW2  / (1 - self.beta1**(epoch+1))\n",
        "        self.vW0hat = self.vW0  / (1 - self.beta2**(epoch+1))\n",
        "        self.vW1hat = self.vW1  / (1 - self.beta2**(epoch+1))\n",
        "        self.vW2hat = self.vW2  / (1 - self.beta2**(epoch+1))\n",
        "        self.W0 = self.W0 - (self.alpha * self.mW0hat / (np.sqrt(self.vW0hat) + self.epsilon))\n",
        "        self.W1 = self.W1 - (self.alpha * self.mW1hat / (np.sqrt(self.vW1hat) + self.epsilon))\n",
        "        self.W2 = self.W2 - (self.alpha * self.mW2hat / (np.sqrt(self.vW2hat) + self.epsilon))\n",
        "        self.b0 = self.b0 - self.alpha * (np.ones((1, self.delta1.shape[0]))@self.delta1)\n",
        "        self.b1 = self.b1 - self.alpha * (np.ones((1, self.delta2.shape[0]))@self.delta2)\n",
        "        self.b2 = self.b2 - self.alpha * (np.ones((1, self.delta3.shape[0]))@self.delta3)\n",
        "    def train(self, epochs, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.T\n",
        "        else:\n",
        "            tmp = list(zip(self.X, self.output))\n",
        "            np.random.shuffle(tmp)\n",
        "            self.X, self.output = zip(*tmp)\n",
        "            self.X = np.array(self.X)\n",
        "            self.output = np.array(self.output)\n",
        "        print(\"Hidden Layer Sigmoid, Output Sigmoid, MSE\")\n",
        "        for i in range(epochs):\n",
        "            for j in range(0, self.T, batch_size):\n",
        "                data = self.X[j:min(j+batch_size, self.T)]\n",
        "                output = self.output[j:min(j+batch_size, self.T)]\n",
        "                self.forward(data)\n",
        "                self.backward(data, output, i)\n",
        "            if (i+1)%(max(epochs//50,1)) == 0:\n",
        "                pred = self.forward(self.X)\n",
        "                print(\"Epoch: \", i+1, \"Cost:\", mean_squared_error(self.output, pred), end=\" \")\n",
        "                print(\"Accuracy:\", \"%.2f\" % (100-err(interpretonehot(self.output), interpretonehot(pred))), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdaWbY_QrS14"
      },
      "source": [
        "### 4. Fungsi Aktivasi Tanh, Sigmoid untuk Layer Output, dan Loss Function MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oCtpNkZarS14"
      },
      "outputs": [],
      "source": [
        "class DigsNeuralNetworkTanhSigmoidMSE:\n",
        "    def __init__(self, dataset, oheoutput, flayer, slayer, learningrate):\n",
        "        np.random.seed(23638168)\n",
        "        self.X = dataset\n",
        "        self.alpha = learningrate\n",
        "        self.T = dataset.shape[0]\n",
        "        self.K = dataset.shape[1]\n",
        "        self.H1 = flayer\n",
        "        self.H2 = slayer\n",
        "        self.N0 = oheoutput.shape[1]\n",
        "        self.output = oheoutput\n",
        "        # INITIALIZE FIRST PARAM\n",
        "        limw0 = np.sqrt(6)/np.sqrt(self.K+self.H1)\n",
        "        self.W0 = np.random.uniform(-limw0, limw0, (self.K, self.H1))\n",
        "        self.b0 = np.random.uniform(-limw0, limw0, (1, self.H1))\n",
        "        # INITIALIZE SECOND PARAM\n",
        "        limw1 = np.sqrt(6)/np.sqrt(self.H1+self.H2)\n",
        "        self.W1 = np.random.uniform(-limw1, limw1, (self.H1, self.H2))\n",
        "        self.b1 = np.random.uniform(-limw1, limw1, (1, self.H2))\n",
        "        # INITIALIZE THIRD PARAM\n",
        "        limw2 = np.sqrt(6)/np.sqrt(self.H2+self.N0)\n",
        "        self.W2 = np.random.uniform(-limw2, limw2, (self.H2, self.N0))\n",
        "        self.b2 = np.random.uniform(-limw2, limw2, (1, self.N0))\n",
        "        #NADAM PARAM\n",
        "        self.mW0 = np.zeros((self.K, self.H1))\n",
        "        self.mW1 = np.zeros((self.H1, self.H2))\n",
        "        self.mW2 = np.zeros((self.H2, self.N0))\n",
        "        self.vW0 = np.zeros((self.K, self.H1))\n",
        "        self.vW1 = np.zeros((self.H1, self.H2))\n",
        "        self.vW2 = np.zeros((self.H2, self.N0))\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, dat=None):\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.Z1 = dat@self.W0\n",
        "        self.A1 = tanh(self.Z1)\n",
        "        self.Z2 = self.A1@self.W1\n",
        "        self.A2 = tanh(self.Z2)\n",
        "        self.Z3 = self.A2@self.W2\n",
        "        self.A3 = sigmoid(self.Z3)\n",
        "        return self.A3\n",
        "    def backward(self, dat=None, output=None, epoch=0):\n",
        "        if output is None:\n",
        "            output = self.output\n",
        "        if dat is None:\n",
        "            dat = self.X\n",
        "        self.delta3 = -1/self.T * ((output - self.A3) * dsigmoid(self.Z3))\n",
        "        self.delta2 = (self.delta3 @ self.W2.T) * derivtanh(self.Z2)\n",
        "        self.delta1 = (self.delta2 @ self.W1.T) * derivtanh(self.Z1)\n",
        "        dcdw0 = dat.T @ self.delta1\n",
        "        dcdw1 = self.A1.T @ self.delta2\n",
        "        dcdw2 = self.A2.T @ self.delta3\n",
        "        self.mW0 = self.beta1 * self.mW0 + (1 - self.beta1) * dcdw0\n",
        "        self.vW0 = self.beta2 * self.vW0 + (1 - self.beta2) * dcdw0**2\n",
        "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dcdw1\n",
        "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * dcdw1**2\n",
        "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dcdw2\n",
        "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * dcdw2**2\n",
        "        self.mW0hat = self.mW0  / (1 - self.beta1**(epoch+1))\n",
        "        self.mW1hat = self.mW1  / (1 - self.beta1**(epoch+1))\n",
        "        self.mW2hat = self.mW2  / (1 - self.beta1**(epoch+1))\n",
        "        self.vW0hat = self.vW0  / (1 - self.beta2**(epoch+1))\n",
        "        self.vW1hat = self.vW1  / (1 - self.beta2**(epoch+1))\n",
        "        self.vW2hat = self.vW2  / (1 - self.beta2**(epoch+1))\n",
        "        self.W0 = self.W0 - (self.alpha * self.mW0hat / (np.sqrt(self.vW0hat) + self.epsilon))\n",
        "        self.W1 = self.W1 - (self.alpha * self.mW1hat / (np.sqrt(self.vW1hat) + self.epsilon))\n",
        "        self.W2 = self.W2 - (self.alpha * self.mW2hat / (np.sqrt(self.vW2hat) + self.epsilon))\n",
        "    def train(self, epochs, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.T\n",
        "        else:\n",
        "            tmp = list(zip(self.X, self.output))\n",
        "            np.random.shuffle(tmp)\n",
        "            self.X, self.output = zip(*tmp)\n",
        "            self.X = np.array(self.X)\n",
        "            self.output = np.array(self.output)\n",
        "        print(\"Hidden Layer Tanh, Output Sigmoid, MSE\")\n",
        "        for i in range(epochs):\n",
        "            for j in range(0, self.T, batch_size):\n",
        "                data = self.X[j:min(j+batch_size, self.T)]\n",
        "                output = self.output[j:min(j+batch_size, self.T)]\n",
        "                self.forward(data)\n",
        "                self.backward(data, output, i)\n",
        "            if (i+1)%(max(epochs//50,1)) == 0:\n",
        "                pred = self.forward(self.X)\n",
        "                print(\"Epoch: \", i+1, \"Cost:\", mean_squared_error(self.output, pred), end=\" \")\n",
        "                print(\"Accuracy:\", \"%.2f\" % (100-err(interpretonehot(self.output), interpretonehot(pred))), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmPNNFUp8Lp9"
      },
      "source": [
        "## Tahap Training dan Akurasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y49mCE2E8TJx"
      },
      "source": [
        "### Sigmoid-Softmax-LogLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQEeQP0_rS16",
        "outputId": "1ff1e635-da3e-4d03-b1d0-d771a2ca9b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden Layer Sigmoid, Output Softmax, Log-Loss\n",
            "Epoch:  1 Cost: 1.8542754488438742 Accuracy: 56.48 %\n",
            "Epoch:  2 Cost: 1.448294688959533 Accuracy: 84.81 %\n",
            "Epoch:  3 Cost: 1.175784040040996 Accuracy: 88.31 %\n",
            "Epoch:  4 Cost: 0.9936444334279915 Accuracy: 89.93 %\n",
            "Epoch:  5 Cost: 0.8629863786210981 Accuracy: 91.15 %\n",
            "Epoch:  6 Cost: 0.7649860213070722 Accuracy: 91.93 %\n",
            "Epoch:  7 Cost: 0.6862840107714709 Accuracy: 93.10 %\n",
            "Epoch:  8 Cost: 0.6220518960978149 Accuracy: 93.54 %\n",
            "Epoch:  9 Cost: 0.5692806720317707 Accuracy: 93.93 %\n",
            "Epoch:  10 Cost: 0.5246426018144641 Accuracy: 94.49 %\n"
          ]
        }
      ],
      "source": [
        "#Untuk E1, gunakan neuron 128, 128 (setidaknya)\n",
        "#Untuk E2, gunakan neuron 784, 784 (setidaknya) (modifikasi argumen ketiga dan keempat di inisialisasi traininst)\n",
        "\n",
        "traininst = DigsNeuralNetworkSigmoidSoftmaxLogloss(images_tr, labels_tr, 128, 128, 0.001)\n",
        "np.seterr('ignore')\n",
        "traininst.train(10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTwwXacF8Y-A"
      },
      "source": [
        "### Tanh-Softmax-LogLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJFGxdnrS17",
        "outputId": "e05dd625-1a92-4d64-8d03-f87f2f29aea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden Layer Tanh, Output Softmax, Log-Loss\n",
            "Epoch:  1 Cost: 0.5745352696834305 Accuracy: 86.20 %\n",
            "Epoch:  2 Cost: 0.3015463079962684 Accuracy: 92.65 %\n",
            "Epoch:  3 Cost: 0.21104172022031029 Accuracy: 94.99 %\n",
            "Epoch:  4 Cost: 0.16891191364598523 Accuracy: 96.55 %\n",
            "Epoch:  5 Cost: 0.1435358713746371 Accuracy: 97.72 %\n",
            "Epoch:  6 Cost: 0.12571241052216117 Accuracy: 98.16 %\n",
            "Epoch:  7 Cost: 0.11224953169626666 Accuracy: 98.44 %\n",
            "Epoch:  8 Cost: 0.10142405010050494 Accuracy: 98.78 %\n",
            "Epoch:  9 Cost: 0.09247092985172102 Accuracy: 98.78 %\n",
            "Epoch:  10 Cost: 0.0848671885883983 Accuracy: 98.89 %\n"
          ]
        }
      ],
      "source": [
        "#Untuk E1, gunakan neuron 128, 128 (setidaknya)\n",
        "#Untuk E2, gunakan neuron 784, 784 (setidaknya) (modifikasi argumen ketiga dan keempat di inisialisasi traininst)\n",
        "\n",
        "traininst = DigsNeuralNetworkTanhSoftmaxLogloss(images_tr, labels_tr, 128, 128, 0.001)\n",
        "np.seterr('ignore')\n",
        "traininst.train(10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4bajLv18b-z"
      },
      "source": [
        "### Sigmoid-Sigmoid-MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4NxCLG_rS19",
        "outputId": "46c03b1c-5b32-4e49-b8d3-d74438164d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden Layer Sigmoid, Output Sigmoid, MSE\n",
            "Epoch:  1 Cost: 0.08896645338068868 Accuracy: 28.38 %\n",
            "Epoch:  2 Cost: 0.08859319237079732 Accuracy: 17.20 %\n",
            "Epoch:  3 Cost: 0.08550220403278172 Accuracy: 26.21 %\n",
            "Epoch:  4 Cost: 0.08169853246635943 Accuracy: 39.18 %\n",
            "Epoch:  5 Cost: 0.07757064334710242 Accuracy: 58.43 %\n",
            "Epoch:  6 Cost: 0.0733244078537239 Accuracy: 70.23 %\n",
            "Epoch:  7 Cost: 0.06904679615483453 Accuracy: 76.96 %\n",
            "Epoch:  8 Cost: 0.06487813413655173 Accuracy: 80.91 %\n",
            "Epoch:  9 Cost: 0.0609400721953412 Accuracy: 82.47 %\n",
            "Epoch:  10 Cost: 0.05734682155531985 Accuracy: 83.36 %\n"
          ]
        }
      ],
      "source": [
        "#Untuk E1, gunakan neuron 128, 128 (setidaknya)\n",
        "#Untuk E2, gunakan neuron 784, 784 (setidaknya) (modifikasi argumen ketiga dan keempat di inisialisasi traininst)\n",
        "\n",
        "traininst = DigsNeuralNetworkSigmoidMSE(images_tr, labels_tr, 128, 128, 0.001)\n",
        "np.seterr('ignore')\n",
        "traininst.train(10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj3dkO7Y8ePh"
      },
      "source": [
        "### Tanh-Sigmoid-MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38jkwQpNrS1-",
        "outputId": "5313caa5-af90-4111-8da2-7d790548a5d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden Layer Tanh, Output Sigmoid, MSE\n",
            "Epoch:  1 Cost: 0.06785613471694339 Accuracy: 64.39 %\n",
            "Epoch:  2 Cost: 0.0465345526541412 Accuracy: 75.01 %\n",
            "Epoch:  3 Cost: 0.036652652817885725 Accuracy: 80.80 %\n",
            "Epoch:  4 Cost: 0.02984331963947203 Accuracy: 87.59 %\n",
            "Epoch:  5 Cost: 0.02521310582415535 Accuracy: 91.71 %\n",
            "Epoch:  6 Cost: 0.022070485610371626 Accuracy: 92.60 %\n",
            "Epoch:  7 Cost: 0.019761368385419226 Accuracy: 93.49 %\n",
            "Epoch:  8 Cost: 0.01796234074446923 Accuracy: 94.32 %\n",
            "Epoch:  9 Cost: 0.01649619576794578 Accuracy: 94.82 %\n",
            "Epoch:  10 Cost: 0.015263174794093105 Accuracy: 95.49 %\n"
          ]
        }
      ],
      "source": [
        "#Untuk E1, gunakan neuron 128, 128 (setidaknya)\n",
        "#Untuk E2, gunakan neuron 784, 784 (setidaknya) (modifikasi argumen ketiga dan keempat di inisialisasi traininst)\n",
        "\n",
        "traininst = DigsNeuralNetworkTanhSigmoidMSE(images_tr, labels_tr, 128, 128, 0.001)\n",
        "np.seterr('ignore')\n",
        "traininst.train(10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kode untuk menghitung akurasi berdasarkan training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 4.507512520868113%\n",
            "This is (most likely) an: 2\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions = traininst.forward(images_tr)\n",
        "labeldesired = interpretonehot(labels_tr)\n",
        "labelpred = interpretonehot(predictions)\n",
        "print(f\"Error: {err(labeldesired, labelpred)}%\")\n",
        "ax = 51\n",
        "print(\"This is (most likely) an:\", labelpred[ax])\n",
        "plt.imshow(images_tr[ax].reshape(int(np.sqrt(images_tr[ax].shape[0])), int(np.sqrt(images_tr[ax].shape[0]))), cmap='gray')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tugas_2_PM_Kelompok 10 [untuk colab].ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "37eb40c3388cfde35488e2d005b0d69ca91ddeff8a429754d4da636d3f888e5e"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
